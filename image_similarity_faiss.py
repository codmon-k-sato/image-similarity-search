# -*- coding: utf-8 -*-
import os
import sys
import glob
import time
import webbrowser
from datetime import datetime
import base64
import io

import numpy as np
from PIL import Image

import torch
import torch.nn as nn
import torchvision.transforms as T
import torchvision.models as models

import faiss  # pip install faiss-cpu

# Google Sheetsé€£æºã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™
# import gspread
# from google.oauth2.service_account import Credentials

# ========================================
# è¨­å®šå¤‰æ•°ï¼ˆã“ã“ã§å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
# ========================================
TOLERANCE = 0.87  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆ1.0ãŒå®Œå…¨ä¸€è‡´ã€æ¨å¥¨: 0.70-0.80ï¼‰
MAX_RESULTS = None  # None ã«ã™ã‚‹ã¨åˆ¶é™ãªã—
MAX_TARGET_IMAGES = None  # Targetç”»åƒã®æœ€å¤§æ•°ï¼ˆNone = å…¨ã¦ä½¿ç”¨ï¼‰

TOP_K = 5  # FAISS ãŒè¿”ã™ä¸Šä½ K ä»¶ï¼ˆå€™è£œæ•°ï¼‰ã€‚æœ€ã‚‚é¡ä¼¼ãª1ä»¶ã‚’ä½¿ã†ãªã‚‰1ã§å¯

# Google Sheetsè¨­å®š
SPREADSHEET_URL = "https://docs.google.com/spreadsheets/d/1opng3SCJc4aJbGnXLB7wGc2NNQYnCe6nGtPRPgjackc/edit?gid=0#gid=0"
SPREADSHEET_ID = "1opng3SCJc4aJbGnXLB7wGc2NNQYnCe6nGtPRPgjackc"

# ãã®ä»–ã®è¨­å®š
IMAGE_EXTENSIONS = (".jpg", ".jpeg", ".png", ".bmp", ".gif", ".tiff", ".webp")
BATCH_SIZE = 25
ENABLE_SPREADSHEET = False  # Google Sheetsé€£æºã‚’ç„¡åŠ¹åŒ–
ENABLE_HTML_REPORT = True

# ========================================

SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]

def setup_google_sheets():
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        credentials_path = os.path.join(script_dir, "credentials.json")
        if not os.path.exists(credentials_path):
            print(f"âŒ Google Service Account credentials file not found: {credentials_path}")
            print("Skipping Google Sheets output.")
            return None
        credentials = Credentials.from_service_account_file(credentials_path, scopes=SCOPES)
        client = gspread.authorize(credentials)
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.sheet1
        return worksheet
    except Exception as e:
        print(f"âŒ Error setting up Google Sheets: {e}")
        return None

def clear_spreadsheet(worksheet):
    try:
        worksheet.clear()
        time.sleep(2)
        return True
    except Exception as e:
        print(f"âŒ Error clearing spreadsheet: {e}")
        return False

def write_to_sheet_batch(worksheet, results, batch_size=BATCH_SIZE):
    try:
        if not clear_spreadsheet(worksheet):
            print("âš ï¸  Failed to clear spreadsheet, but continuing...")
        headers = ["å¯¾è±¡ç”»åƒ", "ãƒãƒƒãƒã—ãŸç”»åƒãƒ‘ã‚¹", "Similarity"]
        worksheet.update(values=[headers], range_name='A1:C1')
        time.sleep(2)
        if not results:
            return True
        total_batches = (len(results) + batch_size - 1) // batch_size
        for i in range(0, len(results), batch_size):
            batch = results[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            batch_data = []
            for result in batch:
                row = [
                    result['target_image'],
                    result['matched_path'],
                    result['similarity']
                ]
                batch_data.append(row)
            if batch_data:
                start_row = 2 + i
                end_row = start_row + len(batch_data) - 1
                range_name = f"A{start_row}:C{end_row}"
                try:
                    worksheet.update(values=batch_data, range_name=range_name)
                    if batch_num < total_batches:
                        time.sleep(2)
                except Exception as batch_error:
                    print(f"âŒ Error writing batch {batch_num}: {batch_error}")
                    time.sleep(5)
                    continue
        return True
    except Exception as e:
        print(f"âŒ Error writing to spreadsheet: {e}")
        return False

def image_to_base64(image_path, max_size=(150, 112)):
    """ç”»åƒã‚’ã‚µãƒ ãƒã‚¤ãƒ«åŒ–ã—ã¦Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    try:
        img = Image.open(image_path)
        img.thumbnail(max_size, Image.Resampling.LANCZOS)
        buffered = io.BytesIO()
        # RGBã«å¤‰æ›ï¼ˆPNGã‚„GIFã®é€éå¯¾å¿œï¼‰
        if img.mode in ('RGBA', 'LA', 'P'):
            rgb_img = Image.new('RGB', img.size, (255, 255, 255))
            if img.mode == 'P':
                img = img.convert('RGBA')
            rgb_img.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)
            img = rgb_img
        img.save(buffered, format="JPEG", quality=75)
        img_str = base64.b64encode(buffered.getvalue()).decode()
        return f"data:image/jpeg;base64,{img_str}"
    except Exception as e:
        print(f"âš ï¸  Failed to encode {image_path}: {e}")
        return ""

def generate_html_report(results):
    print("ğŸ“„ Generating HTML report with embedded images...")
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Image Similarity Results (FAISS)</title>
        <meta charset="UTF-8">
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
            .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 30px; }}
            .summary {{ background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); margin-bottom: 20px; }}
            .result {{ background: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
            .images {{ display: flex; gap: 30px; align-items: flex-start; flex-wrap: wrap; }}
            .image-container {{ text-align: center; flex: 1; min-width: 250px; }}
            .image-container img {{ max-width: 200px; max-height: 200px; }}
            .image-path {{ font-size: 12px; color: #666; word-break: break-all; margin-top: 5px; background: #f8f9fa; padding: 5px; border-radius: 4px; }}
            .distance {{ font-size: 20px; font-weight: bold; margin: 10px 0; padding: 10px; border-radius: 5px; text-align: center; background: #2196F3; color: white; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸ” Image Similarity Results (FAISS)</h1>
            <p>Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>

        <div class="summary">
            <h2>ğŸ“Š Summary</h2>
            <p>Total Matches: {len(results)}</p>
            <p>Tolerance (similarity threshold): {TOLERANCE}</p>
        </div>
    """
    # ãƒ‘ã‚¹ã‚’ç°¡ç•¥åŒ–ã™ã‚‹é–¢æ•°
    script_dir = os.path.dirname(os.path.abspath(__file__))
    def simplify_path(path):
        """çµ¶å¯¾ãƒ‘ã‚¹ã‚’ /target/... ã‚„ /æ¤œç´¢dir/... ã®å½¢å¼ã«ç°¡ç•¥åŒ–"""
        abs_path = os.path.abspath(path)
        # targetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ
        if '/target/' in abs_path:
            return '/target/' + abs_path.split('/target/')[-1]
        # æ¤œç´¢å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ï¼‰
        parent_dir = os.path.dirname(script_dir)
        if parent_dir in abs_path and script_dir not in abs_path:
            # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’å–å¾—
            rel_path = os.path.relpath(abs_path, parent_dir)
            return '/' + rel_path
        # ãã®ä»–ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿
        return os.path.basename(abs_path)

    for i, result in enumerate(results, 1):
        sim = float(result['similarity'])
        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        target_base64 = image_to_base64(result['target_image_path'])
        matched_base64 = image_to_base64(result['matched_path'])

        # ãƒ‘ã‚¹è¡¨ç¤ºã‚’ç°¡ç•¥åŒ–
        target_display_path = simplify_path(result['target_image_path'])
        matched_display_path = simplify_path(result['matched_path'])

        html_content += f"""
        <div class="result">
            <h3>Match #{i} - Similarity: {sim:.3f}</h3>
            <div class="images">
                <div class="image-container">
                    <h4>Target</h4>
                    <img src="{target_base64}" alt="Target Image">
                    <div class="image-path">{target_display_path}</div>
                </div>
                <div class="image-container">
                    <h4>Matched</h4>
                    <img src="{matched_base64}" alt="Matched Image">
                    <div class="image-path">{matched_display_path}</div>
                </div>
            </div>
        </div>
        """
    html_content += """
        <div style="text-align:center; margin:40px 0; padding:20px; background:white; border-radius:10px;">
            <h3>ğŸ‰ Report Generated Successfully!</h3>
            <p>ã“ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”»åƒã‚’åŸ‹ã‚è¾¼ã‚“ã§ã„ã‚‹ãŸã‚ã€å˜ä½“ã§å…±æœ‰å¯èƒ½ã§ã™ã€‚</p>
        </div>
    </body>
    </html>
    """
    # å®Ÿè¡Œæ—¥æ™‚ã”ã¨ã®outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—ï¼ˆrun_search.shã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ï¼‰
    timestamp = os.environ.get('OUTPUT_TIMESTAMP', datetime.now().strftime('%Y%m%d_%H%M%S'))
    output_dir = os.path.join(script_dir, "output", timestamp)
    os.makedirs(output_dir, exist_ok=True)

    report_path = os.path.join(output_dir, f"image_similarity_faiss_report.html")
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        try:
            file_url = f"file://{os.path.abspath(report_path)}"
            webbrowser.open(file_url)
        except Exception:
            pass
        print(f"âœ… HTML report generated: {report_path}")
        return report_path
    except Exception as e:
        print(f"âŒ Error generating HTML report: {e}")
        return None

# --------- ç‰¹å¾´æŠ½å‡º ----------
class FeatureExtractor:
    def __init__(self, device=None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        model = models.resnet50(pretrained=True)
        modules = list(model.children())[:-1]
        self.backbone = nn.Sequential(*modules).to(self.device)
        self.backbone.eval()
        self.transform = T.Compose([
            T.Resize(256),
            T.CenterCrop(224),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
        ])
        for p in self.backbone.parameters():
            p.requires_grad = False

    def extract(self, image_path):
        img = None
        x = None
        try:
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå¤§ãã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            file_size = os.path.getsize(image_path)
            if file_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Šã¯ã‚¹ã‚­ãƒƒãƒ—
                return None

            img = Image.open(image_path).convert("RGB")

            # ç”»åƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå¤§ãã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if img.width > 10000 or img.height > 10000:
                if img:
                    img.close()
                return None

            x = self.transform(img).unsqueeze(0).to(self.device)
            with torch.no_grad():
                feats = self.backbone(x)
            feats = feats.squeeze().cpu().numpy().astype('float32')  # 2048
            norm = np.linalg.norm(feats)
            if norm > 0:
                feats = feats / norm

            # ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
            if img:
                img.close()
            del img, x

            return feats
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ¡ãƒ¢ãƒªã‚’ç¢ºå®Ÿã«è§£æ”¾
            try:
                if img:
                    img.close()
                if x is not None:
                    del x
            except:
                pass
            return None

def get_images_from_dir(dir_path):
    image_paths = []
    for ext in IMAGE_EXTENSIONS:
        image_paths.extend(glob.glob(os.path.join(dir_path, f"*{ext}")))
        image_paths.extend(glob.glob(os.path.join(dir_path, f"*{ext.upper()}")))
    return sorted(image_paths)

def compute_embeddings_for_list(paths, extractor, show_progress=False):
    embeddings = []
    valid_paths = []
    error_count = 0
    for i, p in enumerate(paths):
        if show_progress and i % 50 == 0:
            print(f"   Processed {i}/{len(paths)} (errors: {error_count})")
        try:
            f = extractor.extract(p)
            if f is not None:
                embeddings.append(f)
                valid_paths.append(p)
            else:
                error_count += 1
        except Exception as e:
            error_count += 1
            continue

    if show_progress and error_count > 0:
        print(f"   âš ï¸  Skipped {error_count} problematic images")

    if embeddings:
        return np.vstack(embeddings).astype('float32'), valid_paths
    else:
        return np.array([], dtype='float32').reshape(0,2048), []

# --------- ãƒ¡ã‚¤ãƒ³ ----------
SEARCH_ROOT = sys.argv[1] if len(sys.argv) > 1 else "."

# é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‹•çš„ã«æ§‹ç¯‰
SEARCH_ROOT_ABS = os.path.abspath(SEARCH_ROOT)
EXCLUDED_DIRS = [
    os.path.join(SEARCH_ROOT_ABS, ".nuxt", "dist"),
    os.path.join(SEARCH_ROOT_ABS, "node_modules")
]

script_dir = os.path.dirname(os.path.abspath(__file__))
target_dir = os.path.join(script_dir, "target")

print("=" * 60)
print("ğŸ” Image Similarity (FAISS accelerated)")
print("=" * 60)
print(f"ğŸ“Š Settings:")
print(f"   - Similarity threshold: {TOLERANCE}")
print(f"   - Max Results: {MAX_RESULTS if MAX_RESULTS else 'No limit'}")
print(f"   - Max Target Images: {MAX_TARGET_IMAGES if MAX_TARGET_IMAGES else 'No limit'}")
print(f"   - Search Root: {SEARCH_ROOT}")
print(f"   - Target Directory: {target_dir}")
print(f"   - Spreadsheet Output: {ENABLE_SPREADSHEET}")
print(f"   - HTML Report: {ENABLE_HTML_REPORT}")
print("=" * 60)

if not os.path.exists(target_dir):
    print(f"âŒ Target directory not found: {target_dir}")
    sys.exit(1)

worksheet = None
if ENABLE_SPREADSHEET:
    worksheet = setup_google_sheets()

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåŸ‹ã‚è¾¼ã¿ä½œæˆã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
target_image_paths = get_images_from_dir(target_dir)
if not target_image_paths:
    print("âŒ No target images found.")
    sys.exit(1)

# Targetç”»åƒæ•°ã‚’åˆ¶é™ï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
if MAX_TARGET_IMAGES is not None and len(target_image_paths) > MAX_TARGET_IMAGES:
    print(f"â„¹ï¸  Limiting target images from {len(target_image_paths)} to {MAX_TARGET_IMAGES}")
    target_image_paths = target_image_paths[:MAX_TARGET_IMAGES]

extractor = FeatureExtractor()
print(f"ğŸ§  Extracting target features from {len(target_image_paths)} images...")
target_embeddings, valid_target_paths = compute_embeddings_for_list(target_image_paths, extractor, show_progress=True)
if target_embeddings.shape[0] == 0:
    print("âŒ Failed to compute target embeddings.")
    sys.exit(1)

dim = target_embeddings.shape[1]  # 2048
# FAISS å†…ç©ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆL2 æ­£è¦åŒ–æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦å†…ç©ãŒã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
index = faiss.IndexFlatIP(dim)
print(f"ğŸ“š Adding {target_embeddings.shape[0]} vectors to FAISS index...")
index.add(target_embeddings)  # ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 

# æ¤œç´¢å¯¾è±¡ç”»åƒãƒ‘ã‚¹ã‚’åé›†ï¼ˆåŒåãƒ»åŒéšå±¤ã§æ‹¡å¼µå­é•ã„ã¯1ã¤ã ã‘ï¼‰
search_image_paths = []
seen_basenames = {}  # {(dir_path, basename_without_ext): full_path}
for root, _, files in os.walk(SEARCH_ROOT):
    if any(os.path.abspath(root).startswith(excluded) for excluded in EXCLUDED_DIRS):
        continue
    if os.path.abspath(root) == os.path.abspath(target_dir):
        continue
    for file in files:
        if file.lower().endswith(IMAGE_EXTENSIONS):
            full_path = os.path.join(root, file)
            basename_without_ext = os.path.splitext(file)[0]
            key = (root, basename_without_ext)

            # åŒã˜éšå±¤ãƒ»åŒã˜åå‰ã®ç”»åƒãŒæ—¢ã«ã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if key not in seen_basenames:
                seen_basenames[key] = full_path
                search_image_paths.append(full_path)

print(f"ğŸ” Found {len(search_image_paths)} images to search through.")

results = []
match_count = 0
BATCH_READ = 1  # ã‚¯ã‚¨ãƒªã‚’ãƒãƒƒãƒã§å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘ãˆã‚‹ãŸã‚ï¼‰
all_similarities = []  # ã™ã¹ã¦ã®é¡ä¼¼åº¦ã‚’è¨˜éŒ²

for i in range(0, len(search_image_paths), BATCH_READ):
    batch_paths = search_image_paths[i:i+BATCH_READ]
    batch_num = i//BATCH_READ + 1
    total_batches = (len(search_image_paths) + BATCH_READ - 1)//BATCH_READ

    # 100ç”»åƒã”ã¨ã«é€²æ—è¡¨ç¤º
    if batch_num % 100 == 1 or batch_num == 1:
        print(f"ğŸ” Processing image {i+1}/{len(search_image_paths)}...")

    try:
        batch_embeddings, valid_batch_paths = compute_embeddings_for_list(batch_paths, extractor)
    except Exception as batch_error:
        print(f"   âš ï¸  Image {i+1} failed, skipping...")
        continue
    if batch_embeddings.shape[0] == 0:
        continue
    # FAISS ã«ã‚ˆã‚‹æ¤œç´¢ï¼ˆå†…ç©ãªã®ã§é«˜ã„ã»ã©é¡ä¼¼ï¼‰
    # k = TOP_Kï¼ˆå€™è£œæ•°ï¼‰
    D, I = index.search(batch_embeddings, TOP_K)  # D: (b, k) similarities, I: (b, k) indices
    for bi in range(D.shape[0]):
        if MAX_RESULTS and match_count >= MAX_RESULTS:
            break
        sims = D[bi]  # shape (k,)
        idxs = I[bi]
        # æœ€ã‚‚é¡ä¼¼ãªå€™è£œï¼ˆk=TOP_Kã®ã†ã¡ã®1ã¤ï¼‰ãŒé–¾å€¤ä»¥ä¸Šãªã‚‰è¨˜éŒ²
        best_k = int(np.argmax(sims))
        best_sim = float(sims[best_k])
        best_idx = int(idxs[best_k])

        # ã™ã¹ã¦ã®é¡ä¼¼åº¦ã‚’è¨˜éŒ²
        all_similarities.append(best_sim)
        if best_sim >= TOLERANCE:
            matched_target_path = valid_target_paths[best_idx]
            matched_target_name = os.path.basename(matched_target_path)
            matched_search_path = valid_batch_paths[bi]
            match_count += 1
            print(f"âœ… Match {match_count}: {matched_search_path}  <->  {matched_target_name}  (sim={best_sim:.3f})")
            results.append({
                'target_image': matched_target_name,
                'target_image_path': matched_target_path,
                'matched_path': matched_search_path,
                'similarity': f"{best_sim:.3f}"
            })
            if MAX_RESULTS and match_count >= MAX_RESULTS:
                break

print("ğŸ Search completed.")
print(f"ğŸ“Š Total matches found: {len(results)}")

# é¡ä¼¼åº¦ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
if all_similarities:
    all_similarities_arr = np.array(all_similarities)
    print(f"\nğŸ“ˆ Similarity Statistics:")
    print(f"   - Max similarity: {np.max(all_similarities_arr):.4f}")
    print(f"   - Mean similarity: {np.mean(all_similarities_arr):.4f}")
    print(f"   - Median similarity: {np.median(all_similarities_arr):.4f}")
    print(f"   - Min similarity: {np.min(all_similarities_arr):.4f}")
    print(f"   - Threshold: {TOLERANCE}")
    # ä¸Šä½10ä»¶ã‚’è¡¨ç¤º
    top_10_idx = np.argsort(all_similarities_arr)[-10:][::-1]
    print(f"\nğŸ” Top 10 similarities:")
    for idx in top_10_idx:
        if idx < len(search_image_paths):
            print(f"   - {all_similarities_arr[idx]:.4f}: {search_image_paths[idx]}")

# å‡ºåŠ›
if results:
    if ENABLE_HTML_REPORT:
        report_path = generate_html_report(results)
        if report_path:
            print(f"âœ… HTML report available: {os.path.abspath(report_path)}")
    if ENABLE_SPREADSHEET and worksheet:
        print("\nğŸ“ Writing results to Google Sheets...")
        success = write_to_sheet_batch(worksheet, results)
        if success:
            print(f"ğŸ”— Spreadsheet available: {SPREADSHEET_URL}")
        else:
            print("âŒ Failed to write to spreadsheet.")
else:
    print("â„¹ï¸ No matches found.")

print("\n" + "=" * 60)
print("âœ… Process completed!")
print("=" * 60)
